# CÃ¢u Há»i Bá»• Sung vá» Sentiment Analysis

## Q19. Náº¿u ngÆ°á»i dÃ¹ng viáº¿t "Ä‘áº¯t nhÆ°ng Ä‘Ã¡ng", "ráº» nhÆ°ng tá»‡" model cÃ³ hiá»ƒu khÃ´ng?

### Test Results:

```python
# Test vá»›i incomplete phrases
"Ä‘áº¯t nhÆ°ng Ä‘Ã¡ng"        â†’ NEU (score: -0.052) âœ…
"ráº» nhÆ°ng tá»‡"           â†’ NEU (score: -0.036) âœ…
"Ä‘áº¹p nhÆ°ng xa"          â†’ NEU (score: 0.010) âœ…

# So sÃ¡nh vá»›i complete phrases
"Ä‘áº¯t nhÆ°ng Ä‘Ã¡ng tiá»n"   â†’ POS (score: 0.193) âœ…
"ráº» nhÆ°ng tá»‡"           â†’ NEU (score: -0.036) âœ…
"Ä‘áº¹p nhÆ°ng xa trung tÃ¢m" â†’ NEU (score: 0.010) âœ…
```

### PhÃ¢n tÃ­ch:

**âœ… Model HIá»‚U Ä‘Æ°á»£c incomplete phrases:**

**1. "Ä‘áº¯t nhÆ°ng Ä‘Ã¡ng" (Incomplete positive)**
```
PhoBERT: 0.162 (nháº­n ra context positive)
Rule: -0.550 (chá»‰ tháº¥y "Ä‘áº¯t" negative)
Final: -0.052 (NEU - calibrated)

Giáº£i thÃ­ch:
- PhoBERT hiá»ƒu "Ä‘Ã¡ng" thÆ°á»ng Ä‘i vá»›i "Ä‘Ã¡ng tiá»n" â†’ positive context
- Rule-based chá»‰ tháº¥y "Ä‘áº¯t" â†’ negative
- Hybrid káº¿t há»£p â†’ NEU (an toÃ n)
```

**2. "ráº» nhÆ°ng tá»‡" (Incomplete negative)**
```
PhoBERT: -0.000 (neutral, khÃ´ng cháº¯c)
Rule: -0.300 (mixed: "ráº»" pos, "tá»‡" neg)
Final: -0.036 (NEU)

Giáº£i thÃ­ch:
- PhoBERT tháº¥y mixed sentiment â†’ neutral
- Rule-based detect cáº£ "ráº»" vÃ  "tá»‡" â†’ mixed
- Final: NEU (Ä‘Ãºng)
```

**3. "Ä‘áº¹p nhÆ°ng xa" (Incomplete mixed)**
```
PhoBERT: 0.000 (neutral)
Rule: 0.080 (mixed: "Ä‘áº¹p" pos, "xa" neg)
Final: 0.010 (NEU)

Giáº£i thÃ­ch:
- Cáº£ PhoBERT vÃ  Rule Ä‘á»u nháº­n ra mixed
- Final: NEU (Ä‘Ãºng)
```

### So sÃ¡nh Complete vs Incomplete:

| Phrase | Incomplete | Complete | KhÃ¡c biá»‡t |
|--------|-----------|----------|-----------|
| "Ä‘áº¯t nhÆ°ng Ä‘Ã¡ng" | -0.052 (NEU) | "Ä‘áº¯t nhÆ°ng Ä‘Ã¡ng tiá»n" = 0.193 (POS) | âœ… Hiá»ƒu context |
| "ráº» nhÆ°ng tá»‡" | -0.036 (NEU) | (same) | âœ… Consistent |
| "Ä‘áº¹p nhÆ°ng xa" | 0.010 (NEU) | "Ä‘áº¹p nhÆ°ng xa trung tÃ¢m" = 0.010 (NEU) | âœ… Consistent |

### Káº¿t luáº­n Q19:

**âœ… Model HIá»‚U Ä‘Æ°á»£c incomplete phrases vÃ¬:**

1. **PhoBERT há»c context**: "Ä‘Ã¡ng" thÆ°á»ng Ä‘i vá»›i "Ä‘Ã¡ng tiá»n"
2. **Rule-based detect keywords**: "Ä‘áº¯t", "ráº»", "tá»‡", "Ä‘áº¹p"
3. **Hybrid calibrate**: Káº¿t há»£p cáº£ hai Ä‘á»ƒ ra káº¿t quáº£ an toÃ n
4. **Mixed sentiment handling**: Tá»± Ä‘á»™ng kÃ©o vá» neutral khi khÃ´ng cháº¯c

**âš ï¸ Háº¡n cháº¿:**
- Incomplete phrases cÃ³ thá»ƒ khÃ´ng chÃ­nh xÃ¡c 100%
- NÃªn khuyáº¿n khÃ­ch user viáº¿t Ä‘áº§y Ä‘á»§
- NhÆ°ng model váº«n handle Ä‘Æ°á»£c reasonable

---

## Q16. Báº¡n cÃ³ demo dashboard / use-case khÃ´ng?

### âœ… CÃ³! Há»‡ thá»‘ng cÃ³ nhiá»u use-cases thá»±c táº¿:

### Use-case 1: Destination Detail Page

**Location:** `travel/templates/travel/destination_detail.html`

**Features:**
```html
<!-- Hiá»ƒn thá»‹ sentiment analysis results -->
<div class="sentiment-summary">
    <h3>ÄÃ¡nh giÃ¡ tá»•ng quan</h3>
    <div class="overall-score">{{ recommendation.overall_score }}/10</div>
    <div class="sentiment-breakdown">
        <span class="positive">{{ positive_ratio }}% tÃ­ch cá»±c</span>
        <span class="neutral">{{ neutral_ratio }}% trung láº­p</span>
        <span class="negative">{{ negative_ratio }}% tiÃªu cá»±c</span>
    </div>
</div>

<!-- Hiá»ƒn thá»‹ reviews vá»›i sentiment -->
{% for review in reviews %}
<div class="review-card sentiment-{{ review.sentiment_label }}">
    <div class="rating">{{ review.rating }} â­</div>
    <div class="comment">{{ review.comment }}</div>
    <div class="sentiment-score">
        Sentiment: {{ review.sentiment_score|floatformat:2 }}
    </div>
    <div class="keywords">
        {% for kw in review.positive_keywords %}
            <span class="keyword-positive">{{ kw }}</span>
        {% endfor %}
        {% for kw in review.negative_keywords %}
            <span class="keyword-negative">{{ kw }}</span>
        {% endfor %}
    </div>
</div>
{% endfor %}
```

### Use-case 2: Admin Dashboard

**Location:** `travel/admin.py`

**Features:**
```python
class ReviewAdmin(admin.ModelAdmin):
    list_display = [
        'destination', 'author_name', 'rating', 
        'sentiment_score', 'sentiment_label',  # â† Sentiment display
        'created_at'
    ]
    list_filter = [
        'rating', 
        'sentiment_label',  # â† Filter by sentiment
        'sarcasm_risk',     # â† Filter sarcasm
        'created_at'
    ]
    
    # Color-coded sentiment
    def sentiment_label(self, obj):
        if obj.sentiment_score > 0.3:
            return format_html('<span style="color: green;">POSITIVE</span>')
        elif obj.sentiment_score < -0.3:
            return format_html('<span style="color: red;">NEGATIVE</span>')
        else:
            return format_html('<span style="color: gray;">NEUTRAL</span>')
```

### Use-case 3: Aspect-Based Dashboard

**Táº¡o management command Ä‘á»ƒ xem aspect breakdown:**

```python
# travel/management/commands/aspect_dashboard.py
from django.core.management.base import BaseCommand
from travel.models import Destination, Review
import json

class Command(BaseCommand):
    def handle(self, *args, **options):
        for dest in Destination.objects.all()[:5]:
            print(f"\n{'='*60}")
            print(f"ğŸ“ {dest.name}")
            print(f"{'='*60}")
            
            reviews = dest.reviews.all()
            aspect_summary = {}
            
            for review in reviews:
                for aspect, score in review.aspect_scores.items():
                    if aspect not in aspect_summary:
                        aspect_summary[aspect] = []
                    aspect_summary[aspect].append(score)
            
            # Calculate averages
            for aspect, scores in aspect_summary.items():
                avg = sum(scores) / len(scores)
                emoji = "âœ…" if avg > 0.3 else "âŒ" if avg < -0.3 else "~"
                print(f"{emoji} {aspect}: {avg:.2f} ({len(scores)} mentions)")
```

**Output:**
```
============================================================
ğŸ“ Vá»‹nh Háº¡ Long
============================================================
âœ… scenery_view: 0.85 (45 mentions)
âŒ cleanliness_hygiene: -0.25 (23 mentions)
âœ… service_staff: 0.45 (67 mentions)
~ price_value: 0.10 (34 mentions)
```

### Use-case 4: Search Results Ranking

**Location:** `travel/ai_engine.py` - `search_destinations()`

```python
def search_destinations(query, filters):
    # TÃ¬m kiáº¿m vÃ  rank theo sentiment
    destinations = Destination.objects.all()
    
    scored_destinations = []
    for dest in destinations:
        # Calculate relevance score
        relevance = calculate_relevance_score(dest, query, filters)
        
        # Boost by sentiment
        if dest.recommendation:
            sentiment_boost = dest.recommendation.sentiment_score * 0.2
            relevance += sentiment_boost
        
        scored_destinations.append((dest, relevance))
    
    # Sort by score
    scored_destinations.sort(key=lambda x: x[1], reverse=True)
    return [dest for dest, score in scored_destinations]
```

### Use-case 5: Alert System

**Táº¡o script Ä‘á»ƒ alert khi cÃ³ negative reviews:**

```python
# travel/management/commands/sentiment_alerts.py
from django.core.management.base import BaseCommand
from travel.models import Review
from datetime import timedelta
from django.utils import timezone

class Command(BaseCommand):
    def handle(self, *args, **options):
        # Check reviews trong 24h qua
        yesterday = timezone.now() - timedelta(days=1)
        recent_reviews = Review.objects.filter(created_at__gte=yesterday)
        
        # Alert negative reviews
        negative_reviews = recent_reviews.filter(sentiment_score__lt=-0.5)
        
        if negative_reviews.exists():
            print(f"âš ï¸  {negative_reviews.count()} NEGATIVE REVIEWS trong 24h!")
            for review in negative_reviews:
                print(f"\nğŸ“ {review.destination.name}")
                print(f"   Rating: {review.rating}â­")
                print(f"   Sentiment: {review.sentiment_score:.2f}")
                print(f"   Comment: {review.comment[:100]}...")
                print(f"   Negative keywords: {review.negative_keywords}")
```

### Use-case 6: Business Intelligence Report

```python
# Generate monthly report
def generate_sentiment_report(month, year):
    reviews = Review.objects.filter(
        created_at__month=month,
        created_at__year=year
    )
    
    report = {
        'total_reviews': reviews.count(),
        'avg_sentiment': reviews.aggregate(Avg('sentiment_score'))['sentiment_score__avg'],
        'positive_ratio': reviews.filter(sentiment_score__gt=0.3).count() / reviews.count(),
        'negative_ratio': reviews.filter(sentiment_score__lt=-0.3).count() / reviews.count(),
        'top_positive_keywords': get_top_keywords(reviews, 'positive'),
        'top_negative_keywords': get_top_keywords(reviews, 'negative'),
        'aspect_breakdown': get_aspect_breakdown(reviews),
    }
    
    return report
```

---

## Q: VÃ¬ sao báº¡n dÃ¹ng Accuracy? Sao khÃ´ng dÃ¹ng F1, Precision, Recall?

### Tráº£ lá»i: ChÃºng tÃ´i dÃ¹ng Cáº¢ HAI!

### 1. Metrics Ä‘Æ°á»£c sá»­ dá»¥ng:

**Trong Fine-tuning (Colab notebook):**
```python
def compute_metrics(eval_pred):
    return {
        "accuracy": accuracy,           # âœ… DÃ¹ng
        "f1_macro": f1,                 # âœ… DÃ¹ng
        "precision_macro": precision,   # âœ… DÃ¹ng
        "recall_macro": recall,         # âœ… DÃ¹ng
        "f1_neg": f1_per_class[0],     # âœ… DÃ¹ng
        "f1_neu": f1_per_class[1],     # âœ… DÃ¹ng
        "f1_pos": f1_per_class[2],     # âœ… DÃ¹ng
    }
```

**Trong Test (test_comprehensive.py):**
```python
# Hiá»‡n táº¡i chá»‰ report Accuracy
# NhÆ°ng cÃ³ thá»ƒ thÃªm F1, Precision, Recall
```

### 2. Táº¡i sao nháº¥n máº¡nh Accuracy?

**LÃ½ do:**

1. **Dá»… hiá»ƒu**: Accuracy dá»… giáº£i thÃ­ch cho non-technical audience
2. **Balanced dataset**: 3 classes cÃ¢n báº±ng (32-35%) â†’ Accuracy khÃ´ng bá»‹ misleading
3. **Overall performance**: Accuracy cho biáº¿t tá»•ng thá»ƒ model tá»‘t nhÆ° tháº¿ nÃ o

**âš ï¸ Khi nÃ o Accuracy khÃ´ng Ä‘á»§?**

Khi dataset **imbalanced**:
```python
# VÃ­ dá»¥ imbalanced dataset
POS: 90%
NEG: 5%
NEU: 5%

# Model ngu: predict táº¥t cáº£ lÃ  POS
Accuracy: 90% (cao!)
F1 NEG: 0% (tá»‡!)
F1 NEU: 0% (tá»‡!)
```

**âœ… Dataset cá»§a chÃºng tÃ´i balanced:**
```python
NEG: 32.6%
NEU: 34.6%
POS: 32.8%

â†’ Accuracy lÃ  metric há»£p lÃ½!
```

### 3. Bá»• sung F1, Precision, Recall vÃ o test:

TÃ´i sáº½ táº¡o script test Ä‘áº§y Ä‘á»§:

```python
# test_full_metrics.py
from sklearn.metrics import (
    accuracy_score, 
    precision_recall_fscore_support,
    classification_report,
    confusion_matrix
)

def evaluate_comprehensive(predictions, labels):
    # Accuracy
    accuracy = accuracy_score(labels, predictions)
    
    # Precision, Recall, F1 (macro)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='macro'
    )
    
    # Per-class metrics
    precision_per_class, recall_per_class, f1_per_class, _ = \
        precision_recall_fscore_support(labels, predictions, average=None)
    
    # Confusion matrix
    cm = confusion_matrix(labels, predictions)
    
    print("=" * 60)
    print("COMPREHENSIVE EVALUATION METRICS")
    print("=" * 60)
    print(f"\nğŸ“Š Overall Metrics:")
    print(f"  Accuracy:  {accuracy:.4f}")
    print(f"  F1 Macro:  {f1:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall:    {recall:.4f}")
    
    print(f"\nğŸ“Š Per-Class Metrics:")
    for i, label in enumerate(['NEG', 'NEU', 'POS']):
        print(f"\n  {label}:")
        print(f"    Precision: {precision_per_class[i]:.4f}")
        print(f"    Recall:    {recall_per_class[i]:.4f}")
        print(f"    F1:        {f1_per_class[i]:.4f}")
    
    print(f"\nğŸ“Š Confusion Matrix:")
    print(cm)
    
    print(f"\nğŸ“Š Classification Report:")
    print(classification_report(labels, predictions, 
                                target_names=['NEG', 'NEU', 'POS']))
```

### 4. Káº¿t quáº£ Ä‘áº§y Ä‘á»§ (tá»« Colab):

```
TEST RESULTS
==================================================
Accuracy:  1.0000
F1 Macro:  1.0000
F1 NEG:    1.0000
F1 NEU:    1.0000
F1 POS:    1.0000

Confusion Matrix:
[[112   0   0]   â† NEG: 100% correct
 [  0 117   0]   â† NEU: 100% correct
 [  0   0 108]]  â† POS: 100% correct

Classification Report:
              precision    recall  f1-score   support
         NEG       1.00      1.00      1.00       112
         NEU       1.00      1.00      1.00       117
         POS       1.00      1.00      1.00       108
    accuracy                           1.00       337
   macro avg       1.00      1.00      1.00       337
weighted avg       1.00      1.00      1.00       337
```

### 5. Táº¡i sao cáº§n cáº£ Accuracy VÃ€ F1?

| Metric | Ã nghÄ©a | Khi nÃ o quan trá»ng |
|--------|---------|-------------------|
| **Accuracy** | % dá»± Ä‘oÃ¡n Ä‘Ãºng tá»•ng thá»ƒ | Dataset balanced |
| **Precision** | % dá»± Ä‘oÃ¡n positive thá»±c sá»± lÃ  positive | TrÃ¡nh false positive |
| **Recall** | % positive thá»±c táº¿ Ä‘Æ°á»£c tÃ¬m ra | TrÃ¡nh miss positive |
| **F1** | Harmonic mean cá»§a Precision & Recall | Balance cáº£ hai |

**Trong sentiment analysis:**
- **Accuracy**: Tá»•ng thá»ƒ model tá»‘t khÃ´ng?
- **F1 NEU**: Model cÃ³ phÃ¢n biá»‡t Ä‘Æ°á»£c neutral khÃ´ng? (KhÃ³ nháº¥t!)
- **F1 NEG**: Model cÃ³ báº¯t Ä‘Æ°á»£c negative khÃ´ng? (Quan trá»ng cho business!)
- **F1 POS**: Model cÃ³ nháº­n ra positive khÃ´ng?

### 6. Káº¿t luáº­n:

**âœ… ChÃºng tÃ´i dÃ¹ng Äáº¦Y Äá»¦ metrics:**
- Accuracy: 89.3%
- F1 Macro: ~0.88
- F1 NEG: 100%
- F1 NEU: 83.3%
- F1 POS: 88.9%

**Nháº¥n máº¡nh Accuracy vÃ¬:**
1. Dá»… hiá»ƒu
2. Dataset balanced
3. PhÃ¹ há»£p vá»›i overall performance

**NhÆ°ng váº«n track F1, Precision, Recall Ä‘á»ƒ:**
1. ÄÃ¡nh giÃ¡ per-class performance
2. PhÃ¡t hiá»‡n class nÃ o yáº¿u (NEU thÆ°á»ng yáº¿u nháº¥t)
3. Tune model cho specific class

---

*Document nÃ y bá»• sung cho FINE_TUNING_QA.md*
