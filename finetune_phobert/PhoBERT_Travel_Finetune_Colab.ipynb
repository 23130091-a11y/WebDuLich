{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Fine-tune PhoBERT cho Travel Sentiment Analysis\n",
        "\n",
        "**Th·ªùi gian ch·∫°y: ~5-10 ph√∫t v·ªõi GPU T4**\n",
        "\n",
        "## H∆∞·ªõng d·∫´n:\n",
        "1. V√†o **Runtime > Change runtime type > GPU (T4)**\n",
        "2. Ch·∫°y t·ª´ng cell t·ª´ tr√™n xu·ªëng\n",
        "3. Download model v·ªÅ m√°y ·ªü cu·ªëi notebook"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ C√†i ƒë·∫∑t th∆∞ vi·ªán"
      ],
      "metadata": {
        "id": "install"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_libs"
      },
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t phi√™n b·∫£n m·ªõi nh·∫•t c·ªßa transformers\n",
        "!pip install -q transformers>=4.30.0 datasets scikit-learn pandas accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Check if transformers version is compatible\n",
        "from packaging import version\n",
        "if version.parse(transformers.__version__) < version.parse(\"4.30.0\"):\n",
        "    print(\"\\n‚ö†Ô∏è  Transformers version is old. Upgrading...\")\n",
        "    !pip install -q --upgrade transformers\n",
        "    print(\"‚úÖ Please restart runtime: Runtime > Restart runtime\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ T·∫°o Training Dataset\n",
        "\n",
        "Dataset bao g·ªìm:\n",
        "- **35% Positive**: C√¢u khen r√µ r√†ng\n",
        "- **35% Negative**: C√¢u ch√™ r√µ r√†ng\n",
        "- **30% Neutral/Mixed**: C√¢u trung l·∫≠p, mixed sentiment (QUAN TR·ªåNG!)"
      ],
      "metadata": {
        "id": "dataset_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "\n",
        "# Word banks\n",
        "WORD_BANKS = {\n",
        "    'adj_pos': ['ƒë·∫πp', 'tuy·ªát v·ªùi', 'xu·∫•t s·∫Øc', 't·ªët', '·ªïn', 'ok', 'x·ªãn', 'ƒë·ªânh',\n",
        "                'th√≠ch', 'hay', '·∫•n t∆∞·ª£ng', 'm√™', 'th∆° m·ªông', 'l√£ng m·∫°n'],\n",
        "    'adj_neg': ['t·ªá', 'd·ªü', 'k√©m', 'x·∫•u', 'ch√°n', 'th·∫•t v·ªçng', 't·ªìi', 'b·∫©n'],\n",
        "    'service_pos': ['nh√¢n vi√™n th√¢n thi·ªán', 'ph·ª•c v·ª• t·ªët', 'h·ªó tr·ª£ nhi·ªát t√¨nh',\n",
        "                    'd·ªãch v·ª• chuy√™n nghi·ªáp', 'nh√¢n vi√™n vui v·∫ª'],\n",
        "    'service_neg': ['nh√¢n vi√™n th√°i ƒë·ªô k√©m', 'ph·ª•c v·ª• ch·∫≠m', 'kh√¥ng nhi·ªát t√¨nh',\n",
        "                    'd·ªãch v·ª• t·ªá', 'nh√¢n vi√™n c·ªôc l·ªëc'],\n",
        "    'facility_pos': ['ph√≤ng s·∫°ch s·∫Ω', 'ti·ªán nghi ƒë·∫ßy ƒë·ªß', 'wifi m·∫°nh',\n",
        "                     'ph√≤ng r·ªông r√£i', 'decor ƒë·∫πp'],\n",
        "    'facility_neg': ['ph√≤ng b·∫©n', 'c∆° s·ªü v·∫≠t ch·∫•t c≈©', 'wifi y·∫øu',\n",
        "                     'ph√≤ng nh·ªè', 'xu·ªëng c·∫•p'],\n",
        "    'price_neg': ['ƒë·∫Øt', 'm·∫Øc', 'cao', 'ch√°t', 'kh√¥ng ƒë√°ng ti·ªÅn'],\n",
        "    'hygiene_neg': ['b·∫©n qu√°', 'wc h√¥i', 'r√°c nhi·ªÅu', 'm√πi kh√≥ ch·ªãu'],\n",
        "    'crowd_neg': ['ƒë√¥ng qu√°', 'chen ch√∫c', 'ch·ªù l√¢u', '·ªìn √†o', 'x·∫øp h√†ng d√†i'],\n",
        "}\n",
        "\n",
        "# Templates\n",
        "POSITIVE_TEMPLATES = [\n",
        "    \"ƒê·ªãa ƒëi·ªÉm {adj_pos}, {service_pos}, r·∫•t recommend!\",\n",
        "    \"View {adj_pos}, phong c·∫£nh {adj_pos}, ƒë√°ng ƒëi l·∫Øm!\",\n",
        "    \"{service_pos}, {facility_pos}, s·∫Ω quay l·∫°i!\",\n",
        "    \"Tuy·ªát v·ªùi! {adj_pos}, {adj_pos}, 10 ƒëi·ªÉm!\",\n",
        "    \"C·∫£nh {adj_pos}, ch·ª•p h√¨nh {adj_pos}, th√≠ch l·∫Øm!\",\n",
        "    \"ƒê·ªãa ƒëi·ªÉm {adj_pos}, {service_pos}.\",\n",
        "    \"View {adj_pos}, ƒë√°ng ƒë·ªÉ gh√© thƒÉm.\",\n",
        "    \"{facility_pos}, {adj_pos}.\",\n",
        "]\n",
        "\n",
        "NEGATIVE_TEMPLATES = [\n",
        "    \"D·ªãch v·ª• {adj_neg}, {service_neg}, kh√¥ng bao gi·ªù quay l·∫°i!\",\n",
        "    \"Gi√° {price_neg}, {adj_neg}, th·∫•t v·ªçng!\",\n",
        "    \"{hygiene_neg}, {adj_neg}, kh√¥ng recommend!\",\n",
        "    \"Qu√° {adj_neg}, {service_neg}, ph√≠ ti·ªÅn!\",\n",
        "    \"{crowd_neg}, {service_neg}, {adj_neg}!\",\n",
        "    \"H∆°i {adj_neg}, {service_neg}.\",\n",
        "    \"{adj_neg}, {crowd_neg}.\",\n",
        "]\n",
        "\n",
        "# NEUTRAL/MIXED - QUAN TR·ªåNG NH·∫§T\n",
        "NEUTRAL_TEMPLATES = [\n",
        "    \"C·∫£nh {adj_pos} nh∆∞ng {adj_neg}.\",\n",
        "    \"View {adj_pos}, nh∆∞ng {service_neg}.\",\n",
        "    \"{facility_pos} nh∆∞ng {price_neg}.\",\n",
        "    \"{adj_pos} nh∆∞ng {crowd_neg}.\",\n",
        "    \"Tuy {adj_neg} nh∆∞ng {adj_pos}.\",\n",
        "    \"M·∫∑c d√π {price_neg}, nh∆∞ng {adj_pos}.\",\n",
        "    \"C≈©ng ƒë∆∞·ª£c, kh√¥ng c√≥ g√¨ ƒë·∫∑c bi·ªát.\",\n",
        "    \"T·∫°m ·ªïn, b√¨nh th∆∞·ªùng.\",\n",
        "    \"Kh√° ok, kh√¥ng qu√° {adj_pos}.\",\n",
        "    \"H∆°i {adj_neg}, c√≤n l·∫°i ok.\",\n",
        "    \"Kh√¥ng {adj_neg} l·∫Øm, c≈©ng ƒë∆∞·ª£c.\",\n",
        "    \"Gi√° h∆°i {price_neg}, c√≤n l·∫°i ok.\",\n",
        "    \"{adj_pos}, ch·ªâ h∆°i {adj_neg} th√¥i.\",\n",
        "]\n",
        "\n",
        "# Edge cases - nh√¢n 20 l·∫ßn ƒë·ªÉ tƒÉng weight\n",
        "EDGE_CASES = [\n",
        "    {\"text\": \"Kh√¥ng t·ªá\", \"label\": \"NEU\"},\n",
        "    {\"text\": \"Kh√¥ng d·ªü l·∫Øm\", \"label\": \"NEU\"},\n",
        "    {\"text\": \"C≈©ng kh√¥ng t·ªá\", \"label\": \"NEU\"},\n",
        "    {\"text\": \"Kh√¥ng ƒë·∫πp\", \"label\": \"NEG\"},\n",
        "    {\"text\": \"Kh√¥ng t·ªët l·∫Øm\", \"label\": \"NEG\"},\n",
        "    {\"text\": \"ƒê·∫πp nh∆∞ng ƒë·∫Øt\", \"label\": \"NEU\"},\n",
        "    {\"text\": \"R·∫ª nh∆∞ng d·ªü\", \"label\": \"NEU\"},\n",
        "    {\"text\": \"View ƒë·∫πp nh∆∞ng xa\", \"label\": \"NEU\"},\n",
        "    {\"text\": \"H∆°i ƒë·∫Øt\", \"label\": \"NEU\"},\n",
        "    {\"text\": \"Kh√° ·ªïn\", \"label\": \"NEU\"},\n",
        "    {\"text\": \"T·∫°m ƒë∆∞·ª£c\", \"label\": \"NEU\"},\n",
        "    {\"text\": \"Ok th√¥i\", \"label\": \"NEU\"},\n",
        "    {\"text\": \"B√¨nh th∆∞·ªùng\", \"label\": \"NEU\"},\n",
        "    {\"text\": \"C≈©ng ƒë∆∞·ª£c\", \"label\": \"NEU\"},\n",
        "    {\"text\": \"Gi√° h∆°i cao\", \"label\": \"NEU\"},\n",
        "    {\"text\": \"H∆°i ƒë√¥ng\", \"label\": \"NEU\"},\n",
        "    {\"text\": \"H∆°i ƒë·∫Øt nh∆∞ng ƒë√°ng ti·ªÅn\", \"label\": \"POS\"},\n",
        "    {\"text\": \"Gi√° ƒë·∫Øt nh∆∞ng ƒë√°ng ti·ªÅn\", \"label\": \"POS\"},\n",
        "    {\"text\": \"Dep lam, rat thich\", \"label\": \"POS\"},\n",
        "    {\"text\": \"Te qua, ko bao gio quay lai\", \"label\": \"NEG\"},\n",
        "    {\"text\": \"Cung dc, binh thuong\", \"label\": \"NEU\"},\n",
        "]\n",
        "\n",
        "def fill_template(template: str) -> str:\n",
        "    result = template\n",
        "    for key, words in WORD_BANKS.items():\n",
        "        placeholder = '{' + key + '}'\n",
        "        while placeholder in result:\n",
        "            result = result.replace(placeholder, random.choice(words), 1)\n",
        "    return result\n",
        "\n",
        "def generate_dataset(num_samples: int = 3000):\n",
        "    data = []\n",
        "\n",
        "    # Positive (35%)\n",
        "    for _ in range(int(num_samples * 0.35)):\n",
        "        text = fill_template(random.choice(POSITIVE_TEMPLATES))\n",
        "        data.append({'text': text, 'label': 'POS'})\n",
        "\n",
        "    # Negative (35%)\n",
        "    for _ in range(int(num_samples * 0.35)):\n",
        "        text = fill_template(random.choice(NEGATIVE_TEMPLATES))\n",
        "        data.append({'text': text, 'label': 'NEG'})\n",
        "\n",
        "    # Neutral/Mixed (30%)\n",
        "    for _ in range(int(num_samples * 0.30)):\n",
        "        text = fill_template(random.choice(NEUTRAL_TEMPLATES))\n",
        "        data.append({'text': text, 'label': 'NEU'})\n",
        "\n",
        "    # Add edge cases (x20)\n",
        "    for _ in range(20):\n",
        "        data.extend(EDGE_CASES.copy())\n",
        "\n",
        "    random.shuffle(data)\n",
        "    return data\n",
        "\n",
        "# Generate dataset\n",
        "print(\"Generating dataset...\")\n",
        "data = generate_dataset(3000)\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Label mapping\n",
        "LABEL_MAP = {'NEG': 0, 'NEU': 1, 'POS': 2}\n",
        "df['label_id'] = df['label'].map(LABEL_MAP)\n",
        "\n",
        "# Split\n",
        "train_df = df.sample(frac=0.8, random_state=42)\n",
        "remaining = df.drop(train_df.index)\n",
        "val_df = remaining.sample(frac=0.5, random_state=42)\n",
        "test_df = remaining.drop(val_df.index)\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset created!\")\n",
        "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
        "print(f\"\\nLabel distribution (Train):\")\n",
        "print(train_df['label'].value_counts())"
      ],
      "metadata": {
        "id": "create_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Load PhoBERT Model"
      ],
      "metadata": {
        "id": "load_model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "MODEL_NAME = \"wonrax/phobert-base-vietnamese-sentiment\"\n",
        "ID2LABEL = {0: \"NEG\", 1: \"NEU\", 2: \"POS\"}\n",
        "LABEL2ID = {\"NEG\": 0, \"NEU\": 1, \"POS\": 2}\n",
        "\n",
        "print(\"Loading PhoBERT model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=3,\n",
        "    id2label=ID2LABEL,\n",
        "    label2id=LABEL2ID,\n",
        "    ignore_mismatched_sizes=True,\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model loaded! Parameters: {model.num_parameters():,}\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Prepare Data cho Training"
      ],
      "metadata": {
        "id": "prepare_data_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "def create_dataset(df):\n",
        "    dataset = Dataset.from_pandas(df[['text', 'label_id']])\n",
        "    dataset = dataset.rename_column(\"label_id\", \"labels\")\n",
        "\n",
        "    def tokenize(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "        )\n",
        "\n",
        "    return dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "train_dataset = create_dataset(train_df)\n",
        "val_dataset = create_dataset(val_df)\n",
        "test_dataset = create_dataset(test_df)\n",
        "\n",
        "print(f\"‚úÖ Datasets prepared!\")\n",
        "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "prepare_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Fine-tune Model üöÄ\n",
        "\n",
        "**Th·ªùi gian: ~5-10 ph√∫t v·ªõi GPU T4**"
      ],
      "metadata": {
        "id": "train_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average='macro', zero_division=0\n",
        "    )\n",
        "    _, _, f1_per_class, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average=None, zero_division=0\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1_macro\": f1,\n",
        "        \"f1_neg\": f1_per_class[0],\n",
        "        \"f1_neu\": f1_per_class[1],\n",
        "        \"f1_pos\": f1_per_class[2],\n",
        "    }\n",
        "\n",
        "# Training arguments - Compatible v·ªõi c·∫£ phi√™n b·∫£n c≈© v√† m·ªõi\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./phobert-travel-sentiment\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    eval_strategy=\"epoch\",  # Thay ƒë·ªïi t·ª´ evaluation_strategy\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        "    fp16=True,  # Use FP16 for faster training\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(f\"   Epochs: 3\")\n",
        "print(f\"   Batch size: 16\")\n",
        "print(f\"   Learning rate: 2e-5\")\n",
        "print()\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Evaluate on Test Set"
      ],
      "metadata": {
        "id": "eval_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print(\"üìä Evaluating on test set...\")\n",
        "results = trainer.evaluate(test_dataset)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TEST RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy:  {results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1 Macro:  {results['eval_f1_macro']:.4f}\")\n",
        "print(f\"F1 NEG:    {results['eval_f1_neg']:.4f}\")\n",
        "print(f\"F1 NEU:    {results['eval_f1_neu']:.4f}\")\n",
        "print(f\"F1 POS:    {results['eval_f1_pos']:.4f}\")\n",
        "\n",
        "# Confusion matrix\n",
        "predictions = trainer.predict(test_dataset)\n",
        "preds = np.argmax(predictions.predictions, axis=1)\n",
        "labels = predictions.label_ids\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(labels, preds))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(labels, preds, target_names=['NEG', 'NEU', 'POS']))"
      ],
      "metadata": {
        "id": "evaluate"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Test v·ªõi c√°c c√¢u m·∫´u"
      ],
      "metadata": {
        "id": "test_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test samples\n",
        "test_texts = [\n",
        "    \"ƒê·ªãa ƒëi·ªÉm r·∫•t ƒë·∫πp, view tuy·ªát v·ªùi!\",\n",
        "    \"D·ªãch v·ª• t·ªá, kh√¥ng bao gi·ªù quay l·∫°i!\",\n",
        "    \"C≈©ng ƒë∆∞·ª£c, kh√¥ng c√≥ g√¨ ƒë·∫∑c bi·ªát.\",\n",
        "    \"C·∫£nh ƒë·∫πp nh∆∞ng ƒë√¥ng qu√°.\",\n",
        "    \"Kh√¥ng t·ªá l·∫Øm, t·∫°m ·ªïn.\",\n",
        "    \"H∆°i ƒë·∫Øt nh∆∞ng ƒë√°ng ti·ªÅn.\",\n",
        "    \"Ok th√¥i, b√¨nh th∆∞·ªùng.\",\n",
        "    \"T·∫°m ·ªïn, gi√° h∆°i cao.\",\n",
        "    \"View ƒë·∫πp, nh∆∞ng xa trung t√¢m.\",\n",
        "    \"Ph√≤ng s·∫°ch, nh∆∞ng wifi y·∫øu.\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAMPLE PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for text in test_texts:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "    pred_label = ID2LABEL[np.argmax(probs)]\n",
        "    print(f\"\\n'{text}'\")\n",
        "    print(f\"  ‚Üí {pred_label} (NEG:{probs[0]:.2f}, NEU:{probs[1]:.2f}, POS:{probs[2]:.2f})\")"
      ],
      "metadata": {
        "id": "test_samples"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8Ô∏è‚É£ Save & Download Model"
      ],
      "metadata": {
        "id": "save_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "OUTPUT_DIR = \"./phobert-travel-sentiment-final\"\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"‚úÖ Model saved to {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip model ƒë·ªÉ download\n",
        "!zip -r phobert-travel-sentiment.zip ./phobert-travel-sentiment-final\n",
        "\n",
        "print(\"\\n‚úÖ Model zipped! Click link below to download:\")"
      ],
      "metadata": {
        "id": "zip_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download link\n",
        "from google.colab import files\n",
        "files.download('phobert-travel-sentiment.zip')"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9Ô∏è‚É£ H∆∞·ªõng d·∫´n t√≠ch h·ª£p v√†o Project\n",
        "\n",
        "1. **Download** file `phobert-travel-sentiment.zip`\n",
        "\n",
        "2. **Gi·∫£i n√©n** v√†o th∆∞ m·ª•c project:\n",
        "```\n",
        "WebDuLich-fix-conflic/\n",
        "‚îî‚îÄ‚îÄ travel/\n",
        "    ‚îî‚îÄ‚îÄ models/\n",
        "        ‚îî‚îÄ‚îÄ phobert-travel-sentiment/\n",
        "            ‚îú‚îÄ‚îÄ config.json\n",
        "            ‚îú‚îÄ‚îÄ model.safetensors\n",
        "            ‚îú‚îÄ‚îÄ tokenizer.json\n",
        "            ‚îî‚îÄ‚îÄ ...\n",
        "```\n",
        "\n",
        "3. **Update `ai_engine.py`**:\n",
        "```python\n",
        "# Trong h√†m load_model():\n",
        "finetuned_path = os.path.join(settings.BASE_DIR, 'travel', 'models', 'phobert-travel-sentiment')\n",
        "if os.path.exists(finetuned_path):\n",
        "    model_name = finetuned_path\n",
        "else:\n",
        "    model_name = \"wonrax/phobert-base-vietnamese-sentiment\"\n",
        "```\n",
        "\n",
        "4. **Test**:\n",
        "```bash\n",
        "python test_comprehensive.py\n",
        "```"
      ],
      "metadata": {
        "id": "integration_guide"
      }
    }
  ]
}
