"""
Unified AI Engine for WebDuLich
G·ªôp sentiment analysis v√† recommendation engine th√†nh 1 module duy nh·∫•t

Features:
- PhoBERT sentiment analysis v·ªõi fallback rule-based
- Enhanced rule-based v·ªõi JSON keywords
- Aspect-based sentiment analysis
- Negation, intensifier, downtoner handling
- Sarcasm detection
- Recommendation scoring algorithm
- Caching system t√≠ch h·ª£p
- Search functionality
- Retry mechanism for robustness
"""

import os
import re
import json
import torch
import logging
import hashlib
from collections import Counter, defaultdict
from decimal import Decimal
from typing import Tuple, List, Dict, Any, Optional

from django.db.models import Q, Avg, Count
from django.core.cache import cache
from django.conf import settings
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

logger = logging.getLogger(__name__)

# ==================== LOAD JSON KEYWORDS ====================

def load_json_keywords():
    """Load sentiment and aspect keywords from JSON files"""
    # Try multiple locations (prioritize travel/ directory)
    possible_dirs = [
        os.path.join(settings.BASE_DIR, 'travel'),  # WebDuLich-fix-conflic/travel/ (PREFERRED)
        settings.BASE_DIR,  # WebDuLich-fix-conflic/
        settings.BASE_DIR.parent if hasattr(settings.BASE_DIR, 'parent') else None,  # WebDuLich-fix-conflic/../
    ]
    
    sentiment_data = {}
    aspect_data = {}
    
    for base_dir in possible_dirs:
        if base_dir is None:
            continue
            
        sentiment_file = os.path.join(base_dir, 'travel_sentiment_keywords.json')
        aspect_file = os.path.join(base_dir, 'travel_aspect_keywords.json')
        
        if not sentiment_data and os.path.exists(sentiment_file):
            try:
                with open(sentiment_file, 'r', encoding='utf-8') as f:
                    sentiment_data = json.load(f)
                logger.info(f"Loaded sentiment keywords from {sentiment_file}")
            except Exception as e:
                logger.warning(f"Could not load sentiment keywords from {sentiment_file}: {e}")
        
        if not aspect_data and os.path.exists(aspect_file):
            try:
                with open(aspect_file, 'r', encoding='utf-8') as f:
                    aspect_data = json.load(f)
                logger.info(f"Loaded aspect keywords from {aspect_file}")
            except Exception as e:
                logger.warning(f"Could not load aspect keywords from {aspect_file}: {e}")
        
        if sentiment_data and aspect_data:
            break
    
    if not sentiment_data:
        logger.warning("Sentiment keywords not loaded - using empty dict")
    if not aspect_data:
        logger.warning("Aspect keywords not loaded - using empty dict")
    
    return sentiment_data, aspect_data

# Load keywords at module level
SENTIMENT_DATA, ASPECT_DATA = load_json_keywords()

# ==================== CONSTANTS ====================

# T·ª´ ph·ªß ƒë·ªãnh (negation)
NEGATION_WORDS = [
    'kh√¥ng', 'ko', 'k', 'ch·∫≥ng', 'ch·∫£', 'ƒë·ª´ng', 'ch∆∞a',
    'kh√¥ng ph·∫£i', 'kh√¥ng h·ªÅ', 'kh√¥ng bao gi·ªù', 'ch·∫≥ng bao gi·ªù',
    'kh√¥ng c√≤n', 'ch·∫≥ng c√≤n', 'kh√¥ng th·ªÉ', 'ch∆∞a bao gi·ªù',
    'ch∆∞a t·ª´ng', 'kh√¥ng ƒë∆∞·ª£c', 'ch·∫≥ng ƒë∆∞·ª£c', 'kh√¥ng c√≥',
    'thi·∫øu', 'm·∫•t', 'h·∫øt', 'kh√¥ng th·∫•y', 'ch·∫≥ng th·∫•y'
]

# T·ª´ gi·∫£m nh·∫π (downtoner)
DOWNTONERS = {
    'h∆°i': 0.6,
    'kh√°': 0.6,
    't∆∞∆°ng ƒë·ªëi': 0.6,
    'c≈©ng': 0.6,
    'h∆°i h∆°i': 0.5
}

# T·ª´ tƒÉng c∆∞·ªùng (intensifier)
INTENSIFIERS_STRONG = {
    'c·ª±c k·ª≥': 1.4,
    'c·ª±c k√¨': 1.4,
    'si√™u': 1.4,
    'v√¥ c√πng': 1.4,
    'c·ª±c': 1.4,
    'c·ª±c lu√¥n': 1.4
}

INTENSIFIERS_MEDIUM = {
    'r·∫•t': 1.25,
    'qu√°': 1.25,
    'th·∫≠t s·ª±': 1.25,
    'th·ª±c s·ª±': 1.25,
    'r·∫•t l√†': 1.25,
    'ho√†n to√†n': 1.25,
    'tuy·ªát ƒë·ªëi': 1.25
}

# Merge all intensifiers
INTENSIFIERS = {**INTENSIFIERS_STRONG, **INTENSIFIERS_MEDIUM}

# Sarcasm indicators
SARCASM_INDICATORS = [
    'ha', 'haha', 'hihi', 'hehe',
    ':))', '=))', 'üôÇüôÇ', 'üòè', 'üòÖ',
    'nh·ªâ', 'nh·ªÉ', 'nh·ªü', 'nh√©'
]

# Contrast words - ph·∫ßn sau th∆∞·ªùng quan tr·ªçng h∆°n
CONTRAST_WORDS = [
    'nh∆∞ng', 'tuy nhi√™n', 'tuy', 'm·∫∑c d√π', 'd√π', 'song',
    'th·∫ø nh∆∞ng', 'nh∆∞ng m√†', 'tuy v·∫≠y', 'd√π v·∫≠y', 'd√π sao'
]

# Negative behavior patterns - ch·ªâ b√°o ti√™u c·ª±c m·∫°nh
NEGATIVE_BEHAVIOR_PATTERNS = [
    ('kh√¥ng', 'quay l·∫°i'),
    ('kh√¥ng', 'recommend'),
    ('kh√¥ng', 'gi·ªõi thi·ªáu'),
    ('kh√¥ng', 'ƒë·ªÅ xu·∫•t'),
    ('kh√¥ng', 'n√™n ƒëi'),
    ('kh√¥ng', 'ƒë√°ng'),
    ('ch·∫≥ng', 'quay l·∫°i'),
    ('s·∫Ω kh√¥ng', 'quay l·∫°i'),
    ('l·∫ßn sau', 'kh√¥ng'),
    ('kh√¥ng bao gi·ªù', 'quay l·∫°i'),
    ('kh√¥ng bao gi·ªù', 'ƒë·∫øn'),
    ('th·∫•t v·ªçng', 'ho√†n to√†n'),
    ('ho√†n to√†n', 'th·∫•t v·ªçng'),
]

# Stopwords ti·∫øng Vi·ªát
STOPWORDS = [
    'l√†', 'c·ªßa', 'v√†', 'c√≥', 'ƒë∆∞·ª£c', 'trong', 'v·ªõi', 'cho', 't·ª´', 'n√†y', 'ƒë√≥',
    'm·ªôt', 'c√°c', 'nh·ªØng', 'ƒë·ªÉ', 'khi', 'ƒë√£', 's·∫Ω', 'b·ªã', 'n·∫øu', 'nh∆∞', 'th√¨',
    'm√†', 'hay', 'ho·∫∑c', 'nh∆∞ng', 'v√¨', 'n√™n', 'l·∫°i', 'c√≤n', 'ƒëang'
]


# ==================== TEXT NORMALIZATION ====================

class TextNormalizer:
    """Text normalization v·ªõi teencode v√† slang mapping"""
    
    def __init__(self):
        self.slang_map = SENTIMENT_DATA.get('slang_map', {})
        # Sort by length (longest first) for proper multi-word matching
        self.sorted_slang = sorted(self.slang_map.items(), key=lambda x: len(x[0]), reverse=True)
    
    def normalize(self, text: str) -> str:
        """
        Chu·∫©n h√≥a text:
        - Lowercase
        - Map teencode/slang (longest-first matching)
        - Gi·ªØ d·∫•u ti·∫øng Vi·ªát
        - Normalize whitespace
        """
        if not text:
            return ""
        
        # Lowercase
        text = text.lower()
        
        # Normalize whitespace first
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Replace slang/teencode (longest phrases first)
        # Add padding for easier boundary matching
        text = ' ' + text + ' '
        
        for slang, standard in self.sorted_slang:
            # Add spaces around slang for word boundary matching
            slang_pattern = ' ' + slang + ' '
            standard_replace = ' ' + standard + ' '
            text = text.replace(slang_pattern, standard_replace)
        
        # Remove padding and normalize spaces
        text = text.strip()
        text = re.sub(r'\s+', ' ', text)
        
        return text
    
    def tokenize(self, text: str) -> List[str]:
        """Tokenize text into words"""
        # Keep Vietnamese characters and basic punctuation
        text = re.sub(r'[^\w\s√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë.,!?]', ' ', text)
        tokens = text.split()
        return [t for t in tokens if t]


# ==================== SENTIMENT ANALYZER ====================

class SentimentAnalyzer:
    """
    Enhanced Sentiment Analyzer v·ªõi PhoBERT + Advanced Rule-based fallback
    """
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_loaded = False
        self.normalizer = TextNormalizer()
        
        # Load keywords from JSON
        self.positive_keywords = SENTIMENT_DATA.get('positive', {})
        self.negative_keywords = SENTIMENT_DATA.get('negative', {})
        self.neutral_soft = SENTIMENT_DATA.get('neutral_soft', [])
        
    def load_model(self):
        """Load PhoBERT model (lazy loading)"""
        if self.model_loaded:
            return
        
        try:
            logger.info("Loading PhoBERT sentiment model...")
            
            # Try to load fine-tuned model first
            finetuned_path = os.path.join(settings.BASE_DIR, 'travel', 'models', 'phobert-travel-sentiment-final')
            
            if os.path.exists(finetuned_path):
                model_name = finetuned_path
                logger.info(f"‚úÖ Using FINE-TUNED model from: {finetuned_path}")
            else:
                # Fallback to original model
                model_name = "wonrax/phobert-base-vietnamese-sentiment"
                logger.info(f"‚ö†Ô∏è  Fine-tuned model not found, using original: {model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
            self.model.to(self.device)
            self.model.eval()
            
            self.model_loaded = True
            logger.info(f"PhoBERT model loaded successfully on {self.device}")
            
        except Exception as e:
            logger.error(f"Failed to load PhoBERT model: {e}")
            logger.warning("Will use rule-based sentiment analysis")
            self.model_loaded = False
    
    def analyze(self, text: str) -> Tuple[float, List[str], List[str], Dict[str, Any]]:
        """
        Ph√¢n t√≠ch sentiment c·ªßa text
        
        Returns:
            tuple: (sentiment_score, positive_keywords, negative_keywords, metadata)
                - sentiment_score: float t·ª´ -1 ƒë·∫øn 1
                - positive_keywords: list t·ª´ kh√≥a t√≠ch c·ª±c
                - negative_keywords: list t·ª´ kh√≥a ti√™u c·ª±c
                - metadata: dict ch·ª©a th√¥ng tin ph√¢n t√≠ch (aspects, sarcasm_risk, etc.)
        """
        if not text or not text.strip():
            return 0.0, [], [], {}
        
        # Check cache first
        text_hash = hashlib.md5(text.encode()).hexdigest()[:16]
        cache_key = f'sentiment_v2:{text_hash}'
        
        cached_result = cache.get(cache_key)
        if cached_result is not None:
            return cached_result
        
        # Load model n·∫øu ch∆∞a load
        if not self.model_loaded:
            self.load_model()
        
        # S·ª≠ d·ª•ng PhoBERT n·∫øu c√≥, fallback v·ªÅ rule-based
        if self.model_loaded:
            result = self._phobert_analysis(text)
        else:
            result = self._rule_based_analysis(text)
        
        # Cache result
        cache_timeout = getattr(settings, 'CACHE_TTL', {}).get('sentiment', 86400)
        cache.set(cache_key, result, cache_timeout)
        
        return result
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type((RuntimeError, torch.cuda.OutOfMemoryError)),
        reraise=True
    )
    def _phobert_analysis(self, text: str) -> Tuple[float, List[str], List[str], Dict[str, Any]]:
        """
        PhoBERT sentiment analysis with confidence gating and smart combine.
        
        Strategy:
        - PhoBERT ch·ªâ win khi confidence cao
        - Rule-based win khi c√≥ keyword m·∫°nh ho·∫∑c PhoBERT kh√¥ng t·ª± tin
        - Combine weighted khi c·∫£ hai ƒë·ªÅu c√≥ gi√° tr·ªã
        """
        try:
            # 1. Get rule-based analysis first (always needed for keywords/aspects)
            rule_score, pos_keywords, neg_keywords, metadata = self._rule_based_analysis(text)
            
            # 2. Get PhoBERT prediction
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=256,
                padding=True
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                probabilities = torch.softmax(outputs.logits, dim=-1)
            
            probs = probabilities.cpu().numpy()[0]
            
            # 3. Calculate PhoBERT score with proper scaling
            if len(probs) == 2:  # Binary classification (neg, pos)
                neg_prob, pos_prob = probs
                neu_prob = 0.0
            else:  # 3-class classification (neg, neu, pos)
                neg_prob, neu_prob, pos_prob = probs
            
            # Scale PhoBERT score properly: (pos - neg) * (1 - neu)
            # This reduces score when neutral is high
            phobert_score = (pos_prob - neg_prob) * (1 - neu_prob * 0.5)
            
            # 4. Calculate confidence (top1 - top2)
            probs_sorted = sorted([pos_prob, neu_prob, neg_prob], reverse=True)
            confidence = probs_sorted[0] - probs_sorted[1]
            
            # 5. Smart combine with gating logic
            final_score, combine_method = self._combine_scores(
                rule_score, phobert_score, confidence, 
                len(pos_keywords), len(neg_keywords)
            )
            
            # Update metadata
            metadata['method'] = combine_method
            metadata['phobert_score'] = float(phobert_score)
            metadata['rule_score'] = float(rule_score)
            metadata['confidence'] = float(confidence)
            metadata['probs'] = {
                'pos': float(pos_prob),
                'neu': float(neu_prob),
                'neg': float(neg_prob)
            }
            
            return float(final_score), pos_keywords, neg_keywords, metadata
            
        except Exception as e:
            logger.error(f"PhoBERT analysis failed: {e}")
            return self._rule_based_analysis(text)
    
    def _combine_scores(
        self, 
        rule_score: float, 
        phobert_score: float, 
        confidence: float,
        num_pos_keywords: int,
        num_neg_keywords: int
    ) -> Tuple[float, str]:
        """
        PhoBERT-Primary Combine Strategy (v3.2)
        
        Chi·∫øn l∆∞·ª£c: PhoBERT l√† PRIMARY, Rule-based l√† CALIBRATION
        
        Nguy√™n t·∫Øc:
        1. PhoBERT lu√¥n ƒë√≥ng vai tr√≤ ch√≠nh (55-70% weight)
        2. Rule-based d√πng ƒë·ªÉ calibrate v√† x·ª≠ l√Ω edge cases
        3. Mixed sentiment ‚Üí k√©o v·ªÅ neutral d·ª±a tr√™n PhoBERT
        4. Neutral soft words ‚Üí gi·ªØ g·∫ßn neutral (threshold 0.2)
        
        Returns:
            (final_score, method_name)
        """
        total_keywords = num_pos_keywords + num_neg_keywords
        
        # === CASE 1: Mixed sentiment (c√≥ c·∫£ pos v√† neg keywords) ===
        # ƒê√¢y l√† case quan tr·ªçng nh·∫•t - c·∫ßn k√©o v·ªÅ neutral
        if num_pos_keywords > 0 and num_neg_keywords > 0:
            # PhoBERT quy·∫øt ƒë·ªãnh h∆∞·ªõng, nh∆∞ng dampen m·∫°nh v·ªÅ neutral
            balance = min(num_pos_keywords, num_neg_keywords) / max(num_pos_keywords, num_neg_keywords)
            
            # Damping m·∫°nh h∆°n khi balance cao (keywords c√¢n b·∫±ng)
            damping = 0.40 + (balance * 0.30)
            
            # PhoBERT 60%, rule 40%
            combined = 0.60 * phobert_score + 0.40 * rule_score
            dampened = combined * (1 - damping)
            return max(-1.0, min(1.0, dampened)), "phobert_mixed_neutral_pull"
        
        # === CASE 2: Ch·ªâ c√≥ neutral soft keywords (ok, ƒë∆∞·ª£c, t·∫°m, ·ªïn) ===
        # Rule score th·∫•p (<0.12) th∆∞·ªùng l√† neutral soft only
        if total_keywords > 0 and abs(rule_score) < 0.12:
            # Neutral soft ‚Üí k√©o m·∫°nh v·ªÅ neutral
            # PhoBERT 40%, rule 60%, r·ªìi dampen m·∫°nh
            combined = 0.40 * phobert_score + 0.60 * rule_score
            dampened = combined * 0.35  # Gi·ªØ 35% magnitude ‚Üí g·∫ßn neutral
            return max(-1.0, min(1.0, dampened)), "phobert_neutral_soft_strong_pull"
        
        # === CASE 3: Weak positive keywords (0.12 <= rule < 0.25) ===
        # C√≥ keywords nh∆∞ng y·∫øu ‚Üí dampen v·ªÅ neutral h∆°n
        if total_keywords > 0 and 0.12 <= abs(rule_score) < 0.25:
            # PhoBERT 50%, rule 50%, dampen nh·∫π
            combined = 0.50 * phobert_score + 0.50 * rule_score
            dampened = combined * 0.6  # Gi·ªØ 60%
            return max(-1.0, min(1.0, dampened)), "phobert_weak_signal_calibrated"
        
        # === CASE 4: PhoBERT confidence th·∫•p (<0.20) ===
        if confidence < 0.20:
            # PhoBERT kh√¥ng ch·∫Øc ‚Üí mix v·ªõi rule nhi·ªÅu h∆°n
            # PhoBERT 45%, rule 55%
            final = 0.45 * phobert_score + 0.55 * rule_score
            return max(-1.0, min(1.0, final)), "phobert_low_conf_rule_assist"
        
        # === CASE 5: PhoBERT high confidence (>0.45) ===
        if confidence >= 0.45:
            # PhoBERT r·∫•t t·ª± tin ‚Üí 70% PhoBERT, 30% rule
            final = 0.70 * phobert_score + 0.30 * rule_score
            return max(-1.0, min(1.0, final)), "phobert_dominant_high_conf"
        
        # === CASE 6: Kh√¥ng c√≥ keywords ‚Üí PhoBERT quy·∫øt ƒë·ªãnh ===
        if total_keywords == 0:
            # Kh√¥ng c√≥ domain signal ‚Üí tin PhoBERT nh∆∞ng dampen
            dampened = phobert_score * 0.70
            return max(-1.0, min(1.0, dampened)), "phobert_only_no_keywords"
        
        # === CASE 7: PhoBERT v√† Rule ƒë·ªìng thu·∫≠n (c√πng d·∫•u, c√πng m·∫°nh) ===
        if (phobert_score > 0.2 and rule_score > 0.2) or (phobert_score < -0.2 and rule_score < -0.2):
            # C·∫£ hai ƒë·ªìng √Ω m·∫°nh ‚Üí PhoBERT lead, boost
            # PhoBERT 65%, rule 35%
            final = 0.65 * phobert_score + 0.35 * rule_score
            
            # Boost khi ƒë·ªìng thu·∫≠n r·∫•t m·∫°nh
            if abs(phobert_score) > 0.5 and abs(rule_score) > 0.5:
                final = final * 1.15
            
            return max(-1.0, min(1.0, final)), "phobert_rule_strong_agreement"
        
        # === CASE 8: PhoBERT v√† Rule conflict (kh√°c d·∫•u) ===
        if (phobert_score > 0.15 and rule_score < -0.15) or (phobert_score < -0.15 and rule_score > 0.15):
            # Conflict ‚Üí PhoBERT lead nh∆∞ng dampen m·∫°nh
            # PhoBERT 55%, rule 45%
            final = 0.55 * phobert_score + 0.45 * rule_score
            final = final * 0.65  # Dampen 35%
            return max(-1.0, min(1.0, final)), "phobert_rule_conflict_dampen"
        
        # === DEFAULT: Balanced mix v·ªõi PhoBERT lead ===
        # PhoBERT 60%, rule 40%
        final = 0.60 * phobert_score + 0.40 * rule_score
        return max(-1.0, min(1.0, final)), "phobert_primary_balanced"
    
    def _rule_based_analysis(self, text: str) -> Tuple[float, List[str], List[str], Dict[str, Any]]:
        """Enhanced rule-based sentiment analysis with advanced features"""
        # Normalize text
        text_normalized = self.normalizer.normalize(text)
        
        # Split into sentences
        sentences = self._split_sentences(text_normalized)
        
        total_score = 0.0
        positive_keywords = []
        negative_keywords = []
        aspect_scores = defaultdict(list)
        sarcasm_risk = False
        
        # Check for sarcasm indicators
        for indicator in SARCASM_INDICATORS:
            if indicator in text_normalized:
                sarcasm_risk = True
                break
        
        # Check for negative behavior patterns (strong negative signal)
        negative_behavior_penalty = 0.0
        for pattern in NEGATIVE_BEHAVIOR_PATTERNS:
            if len(pattern) == 2:
                word1, word2 = pattern
                if word1 in text_normalized and word2 in text_normalized:
                    # Check if they appear in order
                    idx1 = text_normalized.find(word1)
                    idx2 = text_normalized.find(word2)
                    if idx1 < idx2 and idx2 - idx1 < 30:  # Within 30 chars
                        negative_behavior_penalty -= 0.5
                        negative_keywords.append(f"{word1} {word2}")
        
        # Check for contrast words and weight accordingly
        has_contrast = any(cw in text_normalized for cw in CONTRAST_WORDS)
        
        for sentence in sentences:
            # Process each sentence
            sentence_score, pos_kw, neg_kw, aspects = self._analyze_sentence(sentence)
            
            total_score += sentence_score
            positive_keywords.extend(pos_kw)
            negative_keywords.extend(neg_kw)
            
            # Collect aspect scores
            for aspect, score in aspects.items():
                aspect_scores[aspect].append(score)
        
        # Apply negative behavior penalty
        total_score += negative_behavior_penalty
        
        # If has contrast word and mixed sentiment, weight toward negative
        # "ƒë·∫πp nh∆∞ng ƒë·∫Øt" ‚Üí ph·∫ßn sau (ƒë·∫Øt) quan tr·ªçng h∆°n
        if has_contrast and positive_keywords and negative_keywords:
            # Reduce positive impact by 20%
            if total_score > 0:
                total_score *= 0.8
        
        # Normalize score to [-1, 1]
        sentiment_score = max(-1.0, min(1.0, total_score))
        
        # Calculate average aspect scores
        avg_aspect_scores = {
            aspect: sum(scores) / len(scores)
            for aspect, scores in aspect_scores.items()
        }
        
        metadata = {
            'aspects': avg_aspect_scores,
            'sarcasm_risk': sarcasm_risk,
            'method': 'rule_based'
        }
        
        return sentiment_score, list(set(positive_keywords)), list(set(negative_keywords)), metadata
    
    def _analyze_sentence(self, sentence: str) -> Tuple[float, List[str], List[str], Dict[str, float]]:
        """
        Analyze a single sentence with advanced rule handling
        
        Returns:
            (score, positive_keywords, negative_keywords, aspect_scores)
        """
        tokens = self.normalizer.tokenize(sentence)
        sentence_score = 0.0
        pos_keywords = []
        neg_keywords = []
        aspect_scores = defaultdict(float)
        
        # Try to match multi-word phrases first (longer phrases have priority)
        all_keywords = {**self.positive_keywords, **self.negative_keywords}
        matched_positions = set()
        
        # Sort keywords by length (descending) to match longer phrases first
        sorted_keywords = sorted(all_keywords.keys(), key=lambda x: len(x.split()), reverse=True)
        
        for keyword in sorted_keywords:
            keyword_tokens = keyword.split()
            keyword_len = len(keyword_tokens)
            
            # Find all occurrences of this keyword
            for i in range(len(tokens) - keyword_len + 1):
                # Skip if any position is already matched
                if any(pos in matched_positions for pos in range(i, i + keyword_len)):
                    continue
                
                # Check if tokens match
                if ' '.join(tokens[i:i+keyword_len]) == keyword:
                    # Mark positions as matched
                    for pos in range(i, i + keyword_len):
                        matched_positions.add(pos)
                    
                    # Get base score
                    base_score = all_keywords[keyword]
                    
                    # Check for modifiers (negation, intensifier, downtoner)
                    modified_score, is_negated = self._apply_modifiers(
                        tokens, i, base_score
                    )
                    
                    # Add to total
                    sentence_score += modified_score
                    
                    # Track keywords
                    if modified_score > 0:
                        if is_negated and base_score < 0:
                            pos_keywords.append(f"kh√¥ng {keyword}")
                        else:
                            pos_keywords.append(keyword)
                    elif modified_score < 0:
                        if is_negated and base_score > 0:
                            neg_keywords.append(f"kh√¥ng {keyword}")
                        else:
                            neg_keywords.append(keyword)
                    
                    # Track aspect
                    aspect = self._get_aspect(keyword)
                    if aspect:
                        aspect_scores[aspect] += modified_score
        
        # Process neutral_soft words as weak positive (ok, ·ªïn, ƒë∆∞·ª£c, t·∫°m...)
        # Score th·∫•p (0.10) ƒë·ªÉ kh√¥ng l√†m c√¢u mixed th√†nh positive
        for i, token in enumerate(tokens):
            if i in matched_positions:
                continue
            if token in self.neutral_soft:
                # Neutral soft words = very weak positive (0.05)
                # G·∫ßn nh∆∞ neutral, ch·ªâ h∆°i positive m·ªôt ch√∫t
                soft_score = 0.05
                
                # Check for negation before neutral_soft
                window_start = max(0, i - 3)
                window = tokens[window_start:i]
                is_negated = any(t in NEGATION_WORDS for t in window)
                
                if is_negated:
                    # "kh√¥ng ok" = weak negative
                    sentence_score -= 0.05
                    neg_keywords.append(f"kh√¥ng {token}")
                else:
                    sentence_score += soft_score
                    pos_keywords.append(token)
                
                matched_positions.add(i)
        
        return sentence_score, pos_keywords, neg_keywords, dict(aspect_scores)
    
    def _apply_modifiers(self, tokens: List[str], keyword_pos: int, base_score: float) -> Tuple[float, bool]:
        """
        Apply negation, intensifier, and downtoner modifiers
        
        Returns:
            (modified_score, is_negated)
        """
        # Check window before keyword (up to 3 tokens)
        window_start = max(0, keyword_pos - 3)
        window = tokens[window_start:keyword_pos]
        
        is_negated = False
        multiplier = 1.0
        
        # Check for negation (highest priority)
        for token in window:
            if token in NEGATION_WORDS:
                is_negated = True
                break
        
        # Check for intensifiers and downtoners
        for token in window:
            if token in INTENSIFIERS:
                multiplier = INTENSIFIERS[token]
                break
            elif token in DOWNTONERS:
                multiplier = DOWNTONERS[token]
                break
        
        # Apply modifications
        if is_negated:
            # Negation: flip sign and reduce magnitude
            # Special case: "kh√¥ng t·ªá" should be weak positive (but not too strong)
            if base_score < 0:
                # "kh√¥ng t·ªá" -> weak positive, capped at 0.20
                modified_score = min(abs(base_score) * 0.5, 0.20)
            else:
                # "kh√¥ng ƒë·∫πp" -> negative
                modified_score = -base_score * 0.8
        else:
            # Apply multiplier
            modified_score = base_score * multiplier
        
        # Clamp to [-1, 1]
        modified_score = max(-1.0, min(1.0, modified_score))
        
        return modified_score, is_negated
    
    def _get_aspect(self, keyword: str) -> Optional[str]:
        """Get aspect category for a keyword"""
        aspects = ASPECT_DATA.get('aspects', {})
        
        for aspect_id, aspect_info in aspects.items():
            if keyword in aspect_info.get('keywords', []):
                return aspect_id
        
        return None
    
    
    def _split_sentences(self, text: str) -> List[str]:
        """T√°ch vƒÉn b·∫£n th√†nh c√°c c√¢u"""
        sentences = re.split(r'[.!?;,\n]', text)
        return [s.strip() for s in sentences if s.strip()]


# ==================== RECOMMENDATION ENGINE ====================

class RecommendationEngine:
    """
    Recommendation Engine cho destinations
    S·ª≠ d·ª•ng Universal Scoring Engine cho ƒëi·ªÉm g·ª£i √Ω
    """
    
    def __init__(self):
        self.sentiment_analyzer = SentimentAnalyzer()
        # Import scoring engine
        from .scoring_engine import get_scoring_engine
        self.scoring_engine = get_scoring_engine()
    
    def search_destinations(self, query: str, filters: Dict[str, Any]) -> List:
        """
        T√¨m ki·∫øm destinations v·ªõi AI scoring
        
        Args:
            query: Search query
            filters: Dict filters (location, travel_type, max_price, etc.)
            
        Returns:
            List of destinations sorted by relevance score
        """
        from .models import Destination
        
        # Build base queryset
        queryset = Destination.objects.select_related('recommendation').prefetch_related('reviews')
        
        # Apply filters
        if filters.get('location'):
            queryset = queryset.filter(location__icontains=filters['location'])
        
        if filters.get('travel_type'):
            queryset = queryset.filter(travel_type__icontains=filters['travel_type'])
        
        # Text search
        if query:
            queryset = queryset.filter(
                Q(name__icontains=query) |
                Q(description__icontains=query) |
                Q(location__icontains=query)
            )
        
        # Calculate relevance scores
        destinations = list(queryset)
        scored_destinations = []
        
        for dest in destinations:
            relevance_score = self._calculate_relevance_score(dest, query, filters)
            scored_destinations.append((dest, relevance_score))
        
        # Sort by score
        scored_destinations.sort(key=lambda x: x[1], reverse=True)
        
        return [dest for dest, score in scored_destinations]
    
    def calculate_destination_score(self, destination) -> Dict[str, float]:
        """
        T√≠nh to√°n ƒëi·ªÉm s·ªë t·ªïng h·ª£p cho destination
        S·ª≠ d·ª•ng Universal Scoring Engine
        
        Returns:
            Dict v·ªõi c√°c scores: overall, review, sentiment, popularity
        """
        return self.scoring_engine.calculate_score(destination)
    
    def _calculate_relevance_score(self, destination, query: str, filters: Dict) -> float:
        """T√≠nh ƒëi·ªÉm relevance cho search results"""
        score = 0.0
        
        # Base recommendation score (50% weight)
        if hasattr(destination, 'recommendation') and destination.recommendation:
            score += destination.recommendation.overall_score * 0.5
        
        # Query relevance (30% weight)
        if query:
            query_lower = query.lower()
            name_match = query_lower in destination.name.lower()
            desc_match = query_lower in (destination.description or '').lower()
            location_match = query_lower in destination.location.lower()
            
            if name_match:
                score += 30
            elif location_match:
                score += 20
            elif desc_match:
                score += 10
        
        # Filter bonus (20% weight)
        if filters.get('travel_type') and filters['travel_type'].lower() in destination.travel_type.lower():
            score += 20
        
        return score
    
    def _calculate_price_score(self, destination) -> float:
        """T√≠nh price competitiveness score d·ª±a tr√™n ph√≠ v√†o c·ªïng"""
        if not destination.entrance_fee:
            return 10.0  # Mi·ªÖn ph√≠ = ƒëi·ªÉm cao nh·∫•t
        
        fee = float(destination.entrance_fee)
        
        if fee == 0:
            return 10.0
        elif fee < 50000:  # < 50k VND
            return 8.0
        elif fee < 100000:  # < 100k VND
            return 6.0
        elif fee < 200000:  # < 200k VND
            return 4.0
        else:
            return 2.0


# ==================== GLOBAL INSTANCES ====================

# Singleton instances
_sentiment_analyzer = None
_recommendation_engine = None

def get_sentiment_analyzer() -> SentimentAnalyzer:
    """Get singleton sentiment analyzer instance"""
    global _sentiment_analyzer
    if _sentiment_analyzer is None:
        _sentiment_analyzer = SentimentAnalyzer()
    return _sentiment_analyzer

def get_recommendation_engine() -> RecommendationEngine:
    """Get singleton recommendation engine instance"""
    global _recommendation_engine
    if _recommendation_engine is None:
        _recommendation_engine = RecommendationEngine()
    return _recommendation_engine


# ==================== PUBLIC API FUNCTIONS ====================

def analyze_sentiment(text: str, rating: int = None) -> Tuple[float, List[str], List[str], Dict[str, Any]]:
    """
    Public API for sentiment analysis with optional post-processing
    
    Args:
        text: Text to analyze
        rating: Optional rating (1-5) for calibration
        
    Returns:
        tuple: (sentiment_score, positive_keywords, negative_keywords, metadata)
    """
    analyzer = get_sentiment_analyzer()
    score, pos_kw, neg_kw, metadata = analyzer.analyze(text)
    
    # POST-PROCESSING (v2.2 improvements)
    
    # 1. Short review boost (< 8 words with weak positive signals)
    word_count = len(text.split())
    if word_count < 8 and 0 < score < 0.15:
        # Short review with weak positive ‚Üí boost slightly
        if any(kw in ['ok', '·ªïn', 'ƒë∆∞·ª£c', 't·∫°m'] for kw in pos_kw):
            score = min(score * 1.5, 0.25)  # Boost to weak positive
            metadata['post_processing'] = 'short_review_boost'
    
    # 2. Rating-based calibration (optional, if rating is provided)
    if rating is not None:
        original_score = score
        
        if rating == 5 and score < 0.5:
            # Rating 5 should be strong positive
            score = max(score, 0.6)
            metadata['calibrated'] = True
            metadata['calibration_reason'] = f'rating_5_boost (from {original_score:.3f})'
        
        elif rating == 4 and score < 0.15:
            # Rating 4 should be at least weak positive
            score = max(score, 0.20)
            metadata['calibrated'] = True
            metadata['calibration_reason'] = f'rating_4_boost (from {original_score:.3f})'
        
        elif rating == 1 and score > -0.5:
            # Rating 1 should be strong negative
            score = min(score, -0.6)
            metadata['calibrated'] = True
            metadata['calibration_reason'] = f'rating_1_adjust (from {original_score:.3f})'
    
    return score, pos_kw, neg_kw, metadata

def search_destinations(query: str, filters: Dict[str, Any]) -> List:
    """
    Public API for destination search
    
    Args:
        query: Search query
        filters: Search filters
        
    Returns:
        List of destinations sorted by relevance
    """
    engine = get_recommendation_engine()
    return engine.search_destinations(query, filters)

def calculate_destination_score(destination) -> Dict[str, float]:
    """
    Public API for destination scoring
    
    Args:
        destination: Destination model instance
        
    Returns:
        Dict with various scores
    """
    engine = get_recommendation_engine()
    return engine.calculate_destination_score(destination)


def get_similar_destinations(destination, limit: int = 4) -> List:
    """
    G·ª£i √Ω ƒë·ªãa ƒëi·ªÉm t∆∞∆°ng t·ª± d·ª±a tr√™n:
    - C√πng lo·∫°i h√¨nh du l·ªãch
    - C√πng khu v·ª±c
    - M·ª©c gi√° t∆∞∆°ng ƒë∆∞∆°ng
    - ƒêi·ªÉm ƒë√°nh gi√° cao
    
    Args:
        destination: Destination hi·ªán t·∫°i
        limit: S·ªë l∆∞·ª£ng g·ª£i √Ω t·ªëi ƒëa
        
    Returns:
        List c√°c destination t∆∞∆°ng t·ª±
    """
    from .models import Destination
    from django.db.models import Q, F, Value, FloatField
    from django.db.models.functions import Abs
    
    # T√¨m c√°c ƒë·ªãa ƒëi·ªÉm kh√°c (kh√¥ng ph·∫£i ƒë·ªãa ƒëi·ªÉm hi·ªán t·∫°i)
    queryset = Destination.objects.select_related('recommendation').exclude(id=destination.id)
    
    similar = []
    
    # 1. C√πng lo·∫°i h√¨nh du l·ªãch (∆∞u ti√™n cao nh·∫•t)
    same_type = queryset.filter(travel_type=destination.travel_type)
    
    # 2. C√πng khu v·ª±c
    same_location = queryset.filter(location=destination.location)
    
    # G·ªôp v√† t√≠nh ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng
    candidates = {}
    
    # ƒêi·ªÉm cho c√πng lo·∫°i h√¨nh
    for dest in same_type[:10]:
        candidates[dest.id] = {'dest': dest, 'score': 50}
    
    # ƒêi·ªÉm cho c√πng khu v·ª±c
    for dest in same_location[:10]:
        if dest.id in candidates:
            candidates[dest.id]['score'] += 40
        else:
            candidates[dest.id] = {'dest': dest, 'score': 40}
    
    # Th√™m ƒëi·ªÉm recommendation
    for dest_id, data in candidates.items():
        dest = data['dest']
        if hasattr(dest, 'recommendation') and dest.recommendation:
            data['score'] += dest.recommendation.overall_score * 0.1
    
    # S·∫Øp x·∫øp theo ƒëi·ªÉm v√† l·∫•y top
    sorted_candidates = sorted(candidates.values(), key=lambda x: x['score'], reverse=True)
    
    return [c['dest'] for c in sorted_candidates[:limit]]


def get_personalized_recommendations(user_preferences: Dict, limit: int = 6) -> List:
    """
    G·ª£i √Ω c√° nh√¢n h√≥a d·ª±a tr√™n s·ªü th√≠ch ng∆∞·ªùi d√πng
    
    Args:
        user_preferences: Dict ch·ª©a s·ªü th√≠ch
            - travel_types: List lo·∫°i h√¨nh y√™u th√≠ch
            - locations: List ƒë·ªãa ƒëi·ªÉm y√™u th√≠ch
            - max_price: Ng√¢n s√°ch t·ªëi ƒëa
        limit: S·ªë l∆∞·ª£ng g·ª£i √Ω
        
    Returns:
        List c√°c destination ph√π h·ª£p
    """
    from .models import Destination
    from django.db.models import Q
    
    queryset = Destination.objects.select_related('recommendation')
    
    # Filter theo s·ªü th√≠ch
    filters = Q()
    
    travel_types = user_preferences.get('travel_types', [])
    if travel_types:
        type_filter = Q()
        for t in travel_types:
            type_filter |= Q(travel_type__icontains=t)
        filters &= type_filter
    
    locations = user_preferences.get('locations', [])
    if locations:
        loc_filter = Q()
        for loc in locations:
            loc_filter |= Q(location__icontains=loc)
        filters &= loc_filter
    
    max_price = user_preferences.get('max_price')
    # B·ªè filter theo gi√° v√¨ ƒë√£ chuy·ªÉn sang entrance_fee
    
    if filters:
        queryset = queryset.filter(filters)
    
    # S·∫Øp x·∫øp theo ƒëi·ªÉm g·ª£i √Ω
    queryset = queryset.order_by('-recommendation__overall_score')
    
    return list(queryset[:limit])


def get_seasonal_recommendations(month: int = None, limit: int = 6) -> List:
    """
    G·ª£i √Ω theo m√πa/th·ªùi ƒëi·ªÉm trong nƒÉm
    
    Args:
        month: Th√°ng (1-12), m·∫∑c ƒë·ªãnh l√† th√°ng hi·ªán t·∫°i
        limit: S·ªë l∆∞·ª£ng g·ª£i √Ω
        
    Returns:
        List c√°c destination ph√π h·ª£p v·ªõi m√πa
    """
    from .models import Destination
    from datetime import datetime
    
    if month is None:
        month = datetime.now().month
    
    queryset = Destination.objects.select_related('recommendation')
    
    # G·ª£i √Ω theo m√πa ·ªü Vi·ªát Nam
    if month in [12, 1, 2]:  # M√πa ƒë√¥ng - T·∫øt
        # ∆Øu ti√™n: Mi·ªÅn B·∫Øc (hoa ƒë√†o), ƒê√† L·∫°t (hoa mai anh ƒë√†o)
        queryset = queryset.filter(
            Q(location__icontains='H√† N·ªôi') |
            Q(location__icontains='Sa Pa') |
            Q(location__icontains='ƒê√† L·∫°t') |
            Q(travel_type__icontains='N√∫i')
        )
    elif month in [3, 4, 5]:  # M√πa xu√¢n
        # ∆Øu ti√™n: Mi·ªÅn Trung, bi·ªÉn
        queryset = queryset.filter(
            Q(location__icontains='ƒê√† N·∫µng') |
            Q(location__icontains='Hu·∫ø') |
            Q(location__icontains='H·ªôi An') |
            Q(travel_type__icontains='Bi·ªÉn')
        )
    elif month in [6, 7, 8]:  # M√πa h√®
        # ∆Øu ti√™n: Bi·ªÉn, ƒë·∫£o
        queryset = queryset.filter(
            Q(location__icontains='Nha Trang') |
            Q(location__icontains='Ph√∫ Qu·ªëc') |
            Q(location__icontains='H·∫° Long') |
            Q(travel_type__icontains='Bi·ªÉn')
        )
    else:  # M√πa thu (9, 10, 11)
        # ∆Øu ti√™n: T√¢y Nguy√™n, mi·ªÅn B·∫Øc
        queryset = queryset.filter(
            Q(location__icontains='ƒê√† L·∫°t') |
            Q(location__icontains='H√† N·ªôi') |
            Q(location__icontains='Ninh B√¨nh') |
            Q(travel_type__icontains='N√∫i')
        )
    
    # S·∫Øp x·∫øp theo ƒëi·ªÉm
    queryset = queryset.order_by('-recommendation__overall_score')
    
    return list(queryset[:limit])


def get_personalized_for_user(user, limit: int = 6) -> List:
    """
    G·ª£i √Ω c√° nh√¢n h√≥a d·ª±a tr√™n s·ªü th√≠ch ƒë√£ l∆∞u c·ªßa user
    
    Args:
        user: User object
        limit: S·ªë l∆∞·ª£ng g·ª£i √Ω
        
    Returns:
        List c√°c destination ph√π h·ª£p v·ªõi s·ªü th√≠ch user
    """
    from .models import Destination
    from users.models import TravelPreference
    from django.db.models import Q
    
    # L·∫•y s·ªü th√≠ch c·ªßa user
    preferences = TravelPreference.objects.filter(user=user)
    
    if not preferences.exists():
        # N·∫øu ch∆∞a c√≥ s·ªü th√≠ch, tr·∫£ v·ªÅ top destinations
        return list(
            Destination.objects.select_related('recommendation')
            .order_by('-recommendation__overall_score')[:limit]
        )
    
    # L·∫•y danh s√°ch travel_type v√† location y√™u th√≠ch
    travel_types = list(preferences.values_list('travel_type', flat=True).distinct())
    locations = list(preferences.values_list('location', flat=True).distinct())
    
    # Build query
    queryset = Destination.objects.select_related('recommendation')
    
    filters = Q()
    
    # Filter theo lo·∫°i h√¨nh y√™u th√≠ch
    if travel_types:
        type_filter = Q()
        for t in travel_types:
            if t:
                type_filter |= Q(travel_type__icontains=t)
        if type_filter:
            filters |= type_filter
    
    # Filter theo ƒë·ªãa ƒëi·ªÉm y√™u th√≠ch
    if locations:
        loc_filter = Q()
        for loc in locations:
            if loc:
                loc_filter |= Q(location__icontains=loc)
        if loc_filter:
            filters |= loc_filter
    
    if filters:
        queryset = queryset.filter(filters)
    
    # S·∫Øp x·∫øp theo ƒëi·ªÉm g·ª£i √Ω
    queryset = queryset.order_by('-recommendation__overall_score')
    
    return list(queryset[:limit])
